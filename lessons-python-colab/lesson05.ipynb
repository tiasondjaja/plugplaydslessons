{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiasondjaja/plugplaydslessons/blob/master/lessons-python-colab/lesson05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8vF0SqerEn4"
      },
      "source": [
        "# Lesson 5: Classification\n",
        "\n",
        "Key Ideas:\n",
        "1. Finding patterns and making predictions about categorical variables\n",
        "    + Overview of the iterative modeling process\n",
        "    + Splitting the dataset into a training set and a validation/testing set\n",
        "    + Finding patterns in the data, using the training set\n",
        "2. Creating simple decision tree classifiers based on patterns observed in data\n",
        "3. Assessing, comparing, and improving models\n",
        "    + Assessing the performance of the model on the training dataset\n",
        "    + Assessing the performance of the model on the test dataset\n",
        "    + Metrics for model assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z47kCF65rEoB"
      },
      "source": [
        "## 1. Finding patterns and making predictions about categorical variables\n",
        "\n",
        "### A quick recap of the mathematical modeling process\n",
        "\n",
        "Recall the math modeling process:\n",
        "+ Step 1: Find patterns in data\n",
        "+ Step 2: Build a model that fits the data relatively well  (e.g., fit a line through the plotted data points)\n",
        "+ Step 3: Assess the model\n",
        "+ Step 4: Repeat, until we have a model that represents the real world process sufficiently well\n",
        "+ Step 5: Use the model to make predictions/decisions\n",
        "\n",
        "In Lesson 3, (1) we found a pattern in the `geyser` data by visualizing the data points in a scatterplot.  (2) From the visualization, and by computing the correlation coefficient between the variables `duration` and `waiting`, we determine that a line is a reasonable type of model.  We then find a line of best fit (one that minimizes MSE).  Assuming that this model is reasonable, we can use the model to predict when the next eruption will take place, given the duration of an eruption that just took place.\n",
        "\n",
        "In the above example, the quantity that we want to predict is `waiting`, the number of minutes until the next eruption, which is a numerical variable. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tACOffvMrEoC"
      },
      "source": [
        "### Predicting a categorical variable\n",
        "\n",
        "Next, we will see an example of models for predicting a categorical variable.  This type of task is called a **classification** task.  Here are some examples of classification tasks:\n",
        "+ Classifying an email as spam or not spam\n",
        "+ Given an image/photograph of an animal, determine the animal's species\n",
        "+ Given an image of a handwritten letter, determine what letter it is\n",
        "+ Given a song, determine its genre\n",
        "+ Given an activity on a website, determine if it is human or a bot\n",
        "+ etc.\n",
        "\n",
        "A binary classification task is one that involves only two possible categories (for example, human vs. bot, or spam vs. not spam).  To keep things simpler, we will focus our attention only on binary classification tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqnTx-BQrEoD"
      },
      "source": [
        "***Example***\n",
        "\n",
        "The dataset below consists of information on 699 tumor samples.  The `id` column contains an identifier for each observed tumor.  The `class` column takes on two values: 2 (indicating that the tumor is benign) and 4 (indicating that the tumor is malignant).  In addition to the `id` and `class` columns, there are nine variables that describe various aspects of each sample.  These variables are discrete numerical variables, and could be thought of as either numerical or categorical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SqWmzs_rEoD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "me31zlbjrEoF"
      },
      "outputs": [],
      "source": [
        "cancerdata = pd.read_csv('../../datasets/cancerdata.csv')\n",
        "cancerdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bpD91nArEoF"
      },
      "source": [
        "Using the above dataset, our goal is to create a model that allows us to predict whether a tumor sample is benign or malignant (that is, whether `class` is 2 or 4), based the values of the nine predictor variables (`clump_thickness`, `uniformity_cellsize`, etc.).\n",
        "\n",
        "Such a model will have to capture a pattern that relates the values of the nine predictor variable to the value of the `class`.  To make sure that we create a model that could be generalized to other tumor data as well (that are not part of this dataset), we need to make sure that the model that we create does not 'overfit' to this particular dataset.\n",
        "\n",
        "For this reason, before we start digging into this dataset to try to find any pattern, we need to first split the dataset into training and test sets.  Each set will contains the same number of columns, but we will randomly assign the 699 rows into the training and the test sets. \n",
        "+ We will use the **training set** to find patterns in the data, to build, and to tune our model.\n",
        "+ We will use the **test set** to assess how well our model generalize to data that it has never \"seen\" before (i.e., to data that were not used to inform how the model was built).\n",
        "\n",
        "\n",
        "### Splitting the dataset into training and test sets\n",
        "\n",
        "There is no one hard rule for how we must split our dataset into training and test sets.  For example, taking 70% of all observations for the training data and the remaining 30% for the test data is acceptable for this example.\n",
        "\n",
        "One of the easiest ways to split a given dataset into training and test sets is to use a tool from the sklearn library.  To do this,\n",
        "+ First store the variable to be predicted into an array (let's call it `Y`), and store the predictor variables into a dataframe (let's call it `X`).\n",
        "+ Import the `train_test_split` function from `sklearn.model_selection`\n",
        "+ Use the `train_test_split` function:\n",
        "    + The first two inputs are `X` and `Y` as specified above\n",
        "    + The third input is `test_size`, which specifies the proportion of all observations that should be assigned to the test set; this can be any number between 0 and 1.\n",
        "    + The fourth input is `random_state`, which can take any nonnegative integer value.  The purpose of this argument is to control the pseudorandom shuffling that is applied to the dataset before it is split.  This makes sure that no matter how many times we run the command, the resulting training and test sets would remain the same instead of changing each time.\n",
        "```\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 1)\n",
        "```\n",
        "+ The outputs of the `train_test_split` function are:\n",
        "    + The first output contains the variables in `X` but only 70% of the observations; this will be the predictor variables in our training set (call it `X_train`)\n",
        "    + The second output contains the variables in `X` but only the remaining 30% of the observations; this will be the predictor variables in our test set (call it `X_test`) \n",
        "    + The third output contains just the `class` values of the observations in the training dataset (call it `y_train`)\n",
        "    + The fourth output contains just the `class` values of the observations in the test dataset (call it `y_test`) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cwhEIGsrEoH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = cancerdata.iloc[ : , 1:10 ]  # we take just the nine predictor variables; all 699 rows\n",
        "Y = cancerdata['class'] # just the class values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-UXhTa_rEoI"
      },
      "outputs": [],
      "source": [
        "# check\n",
        "X_train.shape  # the training dataset has 489 observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lj15OhIyrEoJ"
      },
      "outputs": [],
      "source": [
        "# check\n",
        "X_test.shape # the test dataset has 210 observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsctv7IlrEoK"
      },
      "source": [
        "Above, we see that we have 489 observations in the training dataset (about 70% of 699) and 210 observations in the test dataset (about 30% of 699)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ5yaa03rEoL"
      },
      "source": [
        "### Finding Patterns in Data\n",
        "\n",
        "We are now ready to investigate **the training dataset** to see patterns in the data, using data exploration tools that we learned in lessons 1 and 2.  (We put aside the test dataset until later, once we have built a model, in order to evaluate how well this model performs against data that it hasn't observed during its construction.)  \n",
        "\n",
        "For example, we can start by grouping the rows (the samples) based on whether the sample is benign or malignant (according to the 'class' column), then computing the average value of the nine variables, for each class.\n",
        "\n",
        "Running the code cell below, we see that for a lot of the variables, the averages are smaller among the class-2 rows than the averages among the class-4 rows.  So, we see the pattern that the higher the variable values are, the more likely it is that the sample is that of a malignant tumor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "SSSQoq3HrEoL"
      },
      "outputs": [],
      "source": [
        "cancerdata.groupby('class').agg( 'mean')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXozMBHkrEoL"
      },
      "source": [
        "We could also create data visualizations.  We cannot create a scatterplot to visualize the relationships among all nine variables at once, but we can create a scatterplot for each pair of variables, and coloring each point based on whether the point corresponds to a benign or malignant sample.\n",
        "\n",
        "The scatterplot below tells us that samples whose `clump_thickness` value is 6 or less and whose `uniformity_cellsize` value is 4 or less seem to be mostly benign (`class` = 2).  Points outside of this region seems to correspond mostly to malignant samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VylL1w8IrEoM"
      },
      "outputs": [],
      "source": [
        "sns.relplot( data = cancerdata , x = 'clump_thickness', y = 'uniformity_cellsize' , hue = 'class' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peV-S9ICrEoM"
      },
      "source": [
        "***Exercise***\n",
        "\n",
        "Create a similar scatterplot to visualize other pairs of predictor variables, choosing the hue of each point based on the value of the `class` variable.  Do you observe other useful patterns?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KcxLeimrEoN"
      },
      "outputs": [],
      "source": [
        "# ...\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LfqGllWrEoN"
      },
      "source": [
        "## 2. Creating simple decision tree classifiers based on patterns observed in data\n",
        "\n",
        "With just the above observations, we can create a simple classification model, based only on two variables: `the uniformity_cellsize` and `clump_thickness` values.\n",
        "\n",
        "+ If `clump_thickness` is less than or equal to 6,\n",
        "    + If `uniformity_cellsize` is less than or equal to 4, then predict `class` to be 2 (benign)\n",
        "    + Otherwise (that is, `clump_thickness <= 6` but `uniformity_cellsize > 4`), then predict `class` to be 4 (malignant)\n",
        "+ Otherwise (if `clump_thickness is > 6`, regardless of what `unifromity_cellsize` is), then predict `class` to be 4 (malignant)\n",
        "\n",
        "We can implement this model as a nested if-else statement:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oi9MFAgwrEoO"
      },
      "outputs": [],
      "source": [
        "clump_thickness = 7      # experiment and change this value\n",
        "uniformity_cellsize = 2  # experiment and change this value\n",
        "\n",
        "\n",
        "if clump_thickness <= 6 :\n",
        "    if uniformity_cellsize <= 4:\n",
        "        predicted_class = 2\n",
        "    else:\n",
        "        predicted_class = 4\n",
        "else:\n",
        "    predicted_class = 4\n",
        "\n",
        "print(predicted_class)  # display the predited class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX-oCQ5SrEoO"
      },
      "source": [
        "We can further define a function to make the above code more reusable.  Let's call this function `predict_cancer_class()`\n",
        "+ whose first and second inputs are `clump_thickness` and `uniformity_cellsize` values, respectively.  \n",
        "+ The action done by this function is the sequence of decision implemented by the above nested if-else statement.  \n",
        "+ The output of the function is the predicted class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwl2cFHbrEoP"
      },
      "outputs": [],
      "source": [
        "# define the function / the model\n",
        "def predict_cancer_class( clump_thickness, predicted_class ):\n",
        "    if clump_thickness <= 6 :\n",
        "        if uniformity_cellsize <= 4:\n",
        "            predicted_class = 2\n",
        "        else:\n",
        "            predicted_class = 4\n",
        "    else:\n",
        "        predicted_class = 4\n",
        "        \n",
        "    return( predicted_class )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA9PTdS6rEoP"
      },
      "outputs": [],
      "source": [
        "# check that the function works\n",
        "\n",
        "sample_clump_thickness = 7      # experiment and change this value\n",
        "sample_uniformity_cellsize = 2  # experiment and change this value\n",
        "\n",
        "predict_cancer_class( sample_clump_thickness ,  sample_uniformity_cellsize )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFvBV0fvrEoQ"
      },
      "outputs": [],
      "source": [
        "# check that the function works, example 2\n",
        "\n",
        "sample_clump_thickness = 2      # experiment and change this value\n",
        "sample_uniformity_cellsize = 3  # experiment and change this value\n",
        "\n",
        "predicted_class_for_this_sample = predict_cancer_class( sample_clump_thickness ,  sample_uniformity_cellsize )\n",
        "\n",
        "print( predicted_class_for_this_sample )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmXS2pIorEoQ"
      },
      "source": [
        "### Using the model on the training dataset\n",
        "\n",
        "In the above two code cells, we experimented with using the model using arbitrary sample input values.  Next, we can try to make a prediction on each observation in the training dataset.\n",
        "\n",
        "In the code cell below, we preview the first few rows of the training dataset.  The `clump_thickness` and `uniformity_cellsize` values of the first observation are 3 and 4, respectively.  Our model predicts that the value of `class` is 2 for this observation (benign).\n",
        "\n",
        "We can check if this prediction is correct by viewing the first element of `y_train`, as we did below.  The actual value turns out to be 4, which means that our prediction for this observation was incorrect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-66lCULtrEoQ"
      },
      "outputs": [],
      "source": [
        "display( X_train.head() )\n",
        "\n",
        "# predict class of the first observation (row index 146)\n",
        "predict_cancer_class(3, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo_vHdCTrEoQ"
      },
      "outputs": [],
      "source": [
        "# the actual class value of the first observation:\n",
        "y_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6HIX16jrEoR"
      },
      "source": [
        "In order to get a good sense of how well our model performs, it is not enough to test it on just one observation.  Let's test it on the remaining 488 observations in the training dataset.  \n",
        "\n",
        "We could of course do this by hand, by manually reading and typing the values of `clump_thickness` and `uniformity_cellsize` one by one for each of these 488 observations, but this repetitive task would be tedious and time-consuming.  Instead, we will use a for loop to automate the process.\n",
        "\n",
        "We want our for loop to look up the `clump_thickness` and `uniformity_cellsize` values for each row in `X_train`, then enter them as inputs to our `predict_cancer_class` function.  Therefore, the for loop will go through each item in the list of row indices in `X_train`, which we can obtain using `X_train.index`:\n",
        "\n",
        "```\n",
        "for index in X_train.index :\n",
        "    predict_cancer_class( X_train['clump_thickness'][index] , X_train['uniformity_cellsize'][index] )\n",
        "```\n",
        "The above code is a good start, but we are not done.  Note that the predictions are currently not being stored anywhere!  Therefore, before we start the loop, we will first setup an \"empty\" array that contains just enough space to store the predicted values; let's call this array `y_train_predicted`.\n",
        "```\n",
        "y_train_predicted = np.zeros( y_train.shape ).astype(int)\n",
        "y_train_predicted = pd.Series(y_train_predicted)\n",
        "y_train_predicted.index = y_train.index\n",
        "```\n",
        "In the above code, \n",
        "+ We use the function `np.zeros` to create an array called `y_train_predicted` whose values are all zeros.  The array will have the same number of elements as `y_train`.  (We add `.astype(int)` because the value that we want to predict is 2 or 4, integers, so we set this placeholder to hold integer values as well.) This first line is the most important step; the next two lines are not conceptually important but makes things a bit easier technically.\n",
        "+ Since `y_train` is not just an array but in fact something called a \"pandas Series\", we convert `y_train_predicted` into a pandas Series as well, using the `pd.Series` function.\n",
        "+ Finally, we want to make the row indices of `y_train_predicted` consistent with the row indices of `X_train` and `y_train`.  We do this using: `y_train_predicted.index = y_train.index`\n",
        "\n",
        "\n",
        "Finally, we can store the predictions of our model in the `y_train_predicted` by modifying the loop to be as follows.\n",
        "```\n",
        "for index in X_train.index :\n",
        "    y_train_predicted[index] = predict_cancer_class( X_train['clump_thickness'][index] , X_train['uniformity_cellsize'][index] )\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cP6cD8zrEoR"
      },
      "outputs": [],
      "source": [
        "y_train_predicted = np.zeros( y_train.shape ).astype(int)\n",
        "y_train_predicted = pd.Series(y_train_predicted)\n",
        "y_train_predicted.index = y_train.index\n",
        "\n",
        "# make a prediction for the value of class, for each row in the training dataset.\n",
        "for index in X_train.index :\n",
        "    y_train_predicted[index] = predict_cancer_class( X_train['clump_thickness'][index] , X_train['uniformity_cellsize'][index] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_C12SberEoR"
      },
      "outputs": [],
      "source": [
        "# check\n",
        "y_train_predicted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybB7r8f2rEoR"
      },
      "source": [
        "## 3. Assessing, comparing, and improving models\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CAHNtGbrEoS"
      },
      "source": [
        "### 3.1. Assessing the performance of the model on the training dataset\n",
        "\n",
        "#### Counting the number of correct predictions\n",
        "Above, we have stored our predictions in `y_train_predicted`.  We can compare this value to the actual class values in `y_train`.  One way to assess the quality of our model is simply to count how many of the predictions we get right.\n",
        "\n",
        "We can use \n",
        "```\n",
        "y_train_predicted == y_train\n",
        "```\n",
        "(with two equal signs!) to check whether the elements of `y_train_predicted` is equal to the corresponding elements of `y_train`.  Recall that `y_train_predicted` and `y_train` each has 489 values; the result of the above line will be 489 boolean values (`True` or `False`), depending on whether or not the corresponding predicted value is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "n6xMydMlrEoS"
      },
      "outputs": [],
      "source": [
        "y_train_predicted == y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOxLOBYVrEoS"
      },
      "source": [
        "Python treats `False` as the value 0 and `True` as 1, therefore we can use `sum( y_train_predicted == y_train )` to count the number of correct predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT1_5lCarEoS"
      },
      "outputs": [],
      "source": [
        "num_correct = sum( y_train_predicted == y_train )\n",
        "print(num_correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaeLvTn2rEoT"
      },
      "source": [
        "#### Computing accuracy\n",
        "\n",
        "A better measure of the model performance would be to compute the model's **accuracy**, which is the ratio of the number of correct predictions and the number of all predictions:\n",
        "\n",
        "$$ \\text{ accuracy } = \\frac{\\text{ number of correct predictions} }{ \\text{number of all predictions} }$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8Wme1UurEoT"
      },
      "outputs": [],
      "source": [
        "num_all = len( y_train_predicted )  # this should be the same as len( y_train )\n",
        "accuracy = num_correct / num_all    # ratio of num correct to num of all predictions\n",
        "print(num_correct)\n",
        "print(num_all)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRvB9axGrEoT"
      },
      "source": [
        "That is, the accuracy of our model on the training dataset is about 85.7%.  This is not too bad for a simple first model!\n",
        "\n",
        "Here, we use **accuracy** as the 'score' or the 'metric' we use to assess our model; the closer it is to 100\\%, the better.  Accuracy is not the only assessment metric that we could use; we will see a few other other metrics in Section 3.3.\n",
        "\n",
        "\n",
        "### 3.2. Assessing the performance of the model on the test dataset\n",
        "\n",
        "We have seen that our model performs relatively well on the training dataset, making about 85.7% correct predictions among the data that it has observed.  Next, we want to see how well the model generalizes: how well it performs on data that it has not observed during the model-building stage.  This step is very important, becase models that only perform well on data that it has seen isn't useful.\n",
        "\n",
        "Recall that we did the following to make predictions on the training dataset:\n",
        "```\n",
        "y_train_predicted = np.zeros( y_train.shape ).astype(int)\n",
        "y_train_predicted = pd.Series(y_train_predicted)\n",
        "y_train_predicted.index = y_train.index\n",
        "\n",
        "# make a prediction for the value of class, for each row in the training dataset.\n",
        "for index in X_train.index :\n",
        "    y_train_predicted[index] = predict_cancer_class( X_train['clump_thickness'][index] , X_train['uniformity_cellsize'][index] )\n",
        "```\n",
        "\n",
        "We can repeat this procedure, but applying the `predict_cancer_class()` function on `X_test` instead of `X_train`; we will also adjust the names of the various lists accordingly, and compute the accuracy of the prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qm7pN3pVrEoT"
      },
      "outputs": [],
      "source": [
        "y_test_predicted = np.zeros( y_test.shape ).astype(int)   # Setup a 'blank' array to hold the model output on the test data\n",
        "y_test_predicted = pd.Series(y_test_predicted)\n",
        "y_test_predicted.index = y_test.index\n",
        "\n",
        "# make a prediction for the value of class, for each row in the test dataset.\n",
        "for index in X_test.index :\n",
        "    y_test_predicted[index] = predict_cancer_class( X_test['clump_thickness'][index] , X_test['uniformity_cellsize'][index] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibFdZQGHrEoU"
      },
      "outputs": [],
      "source": [
        "# preview the result\n",
        "y_test_predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG_lqL7ErEoU"
      },
      "outputs": [],
      "source": [
        "# compute the accuracy of this prediction\n",
        "\n",
        "num_correct_test = sum( y_test_predicted == y_test )  # count how many predictions are correct\n",
        "print( num_correct_test )\n",
        "\n",
        "num_all_test = len( y_test )  # count the total number of predictions\n",
        "print( num_all_test )\n",
        "\n",
        "accuracy_test = num_correct_test/ num_all_test   # compute accuracy\n",
        "print(accuracy_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66ohaU9frEoU"
      },
      "source": [
        "The accuracy of our model on the test dataset is about 86.2%, which is comparable (and slightly better) than the accuracy of the model on the training dataset.  This is great!\n",
        "\n",
        "\n",
        "In general, we might expect the model to perform slightly worse on the test dataset.  If the model performs a lot worse on the test dataset, then the model is \"overfitted\" to the training dataset: it captures patterns in the training dataset very well but fails to capture more general patterns that might arise in the test dataset.\n",
        "\n",
        "Of course, we might want to tweak our model (or consider a different model) to achieve higher accuracies on both the training and test datasets.  This will be your task in this lesson's final exercises."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzYLiCQTrEoU"
      },
      "source": [
        "### 3.3. Metrics for model assessment\n",
        "\n",
        "Above, we assessed a model's performance by computing its accuracy.  In some cases, however, accuracy alone does not sufficiently reflect the quality of a model.  Consider the following example.\n",
        "\n",
        "***Motivating Example***\n",
        "\n",
        "Consider a classification task for detecting a rare disease: 0 if the disease is not present (i.e., a negative case for the disease) and 1 otherwise (i.e., a positive case).  Suppose that our test dataset contains a total of 10000 observations, with only 5 positive positive cases and 9995 negative cases.\n",
        "\n",
        "Let us construct a very simple prediction model that always predicts 0 (that the disease is not present).  Clearly, this model is useless!  However, the accuracy of this model on the test dataset is very high:  Since it always predicts 0 and the test dataset contains 9995 negative cases, then the accuracy is $\\frac{9995}{10000} = 0.9995$ or 99.95\\%.\n",
        "\n",
        "In this above example, the model correctly identifies 100\\% of the 99995 negative cases but 0\\% of the 5 positive cases!  Since the point is to help identify occurences of this rare disease, then for this case study, a good model should do a better job identifying the positive cases.\n",
        "\n",
        "*****\n",
        "\n",
        "This example motivates the need for additional metrics that helps capture how well a classification model identify each possible outcome.  \n",
        "\n",
        "#### First, some terminology\n",
        "\n",
        "Given a binary classification task, choose one outcome as the \"positive\" outcome and the other as the \"negative\" outcome.  For example, the presence of a rare disease or a malignant tumor might the the \"positive\" outcome (a positive case, even though its undesireable).\n",
        "\n",
        "We define the following terms:\n",
        "+ **True Positive**: The model predicts the positive case, and this prediction is correct.\n",
        "+ **False Positive**: The model predicts the positive case, and this prediction is incorrect (it is actually a negative case).\n",
        "+ **True Negative**: The model predicts the negative case, and this prediction is correct.\n",
        "+ **False Negative**: The model predicts the negative case, and this prediction is incorrect (it is actually a positive case).\n",
        "\n",
        "Instead of only counting the number of correct predictions made by a model as we did previously, we can also count the number of True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN).\n",
        "\n",
        "We can summarize the above information in a **confusion matrix**: a table with 2 rows and 2 columns where \n",
        "+ The rows correspond to the actual values (positive or negative)\n",
        "+ The columns correspond to the predicted values (positive or negative)\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "    <td>\n",
        "    </td>\n",
        "    <td>\n",
        "    </td>\n",
        "    <th colspan = 2 style=\"text-align: center\">Predicted\n",
        "    </th>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>\n",
        "    </td>\n",
        "    <td>\n",
        "    </td>\n",
        "    <th>Positive\n",
        "    </th>\n",
        "    <th>Negative\n",
        "    </th>\n",
        "</tr>\n",
        "<tr>\n",
        "    <th rowspan = 2> Actual\n",
        "    </th>\n",
        "    <th>Positive\n",
        "    </th>\n",
        "    <td style=\"text-align: left\">TP\n",
        "    </td>\n",
        "    <td style=\"text-align: left\">FN\n",
        "    </td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <th>Negative\n",
        "    </th>\n",
        "    <td style=\"text-align: left\">FP\n",
        "    </td>\n",
        "    <td style=\"text-align: left\">TN\n",
        "    </td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "In terms of our new vocabulary,\n",
        "$$ \\text{accuracy} = \\frac{\\text{ number of correct predictions} }{ \\text{number of all predictions} } = \\frac{\\text{TP}+\\text{TN}}{\\text{TP}+\\text{TN}+\\text{FP}+\\text{FN}}.$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN4GPV2XrEoU"
      },
      "source": [
        "***Motivating Example (continued)***\n",
        "\n",
        "Let's consider again the above example: we have a total of 10000 observations, with only 5 positive positive cases and 9995 negative cases, along with a model that makes 10000 negative predictions and 0 positive ones.  The model makes 9995 correct negative predictions and 5 incorrect negative predictions.  Therefore,\n",
        "+ The number of True Positives (TP) = 0\n",
        "+ The number of False Positives (FP) = 0\n",
        "+ The number of True Negatives (TN) = 9995\n",
        "+ The number of False Negatives (FN) = 5\n",
        "\n",
        "(These four numbers should sum up to 10000, the total number of observations in the dataset.)\n",
        "\n",
        "The corresponding confusion matrix:\n",
        "<table>\n",
        "<tr>\n",
        "    <td>\n",
        "    </td>\n",
        "    <td>\n",
        "    </td>\n",
        "    <th colspan = 2 style=\"text-align: center\">Predicted\n",
        "    </th>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>\n",
        "    </td>\n",
        "    <td>\n",
        "    </td>\n",
        "    <th>Positive\n",
        "    </th>\n",
        "    <th>Negative\n",
        "    </th>\n",
        "</tr>\n",
        "<tr>\n",
        "    <th rowspan = 2> Actual\n",
        "    </th>\n",
        "    <th>Positive\n",
        "    </th>\n",
        "    <td style=\"text-align: left\">TP = 0\n",
        "    </td>\n",
        "    <td style=\"text-align: left\">FN = 5\n",
        "    </td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <th>Negative\n",
        "    </th>\n",
        "    <td style=\"text-align: left\">FP = 0\n",
        "    </td>\n",
        "    <td style=\"text-align: left\">TN = 9995\n",
        "    </td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "The accuracy of this prediction is\n",
        "$$ \\text{accuracy} = \\frac{\\text{TP}+\\text{TN}}{\\text{TP}+\\text{TN}+\\text{FP}+\\text{FN}} = \\frac{0 + 9995}{0+ 9995 + 0 + 5} = \\frac{9995}{10000} = 0.9995$$\n",
        "or 99.95\\%.\n",
        "\n",
        "\n",
        "Earlier, we noted that if we only look at the cases that are actually negative, then the model correctly predicts 100% of them.  Here, \"100%\" is computed as follows\n",
        "$$ \\frac{ \\text{number of all negative cases that are correctly predicted as negative} }{ \\text{number of all cases that are actually negative} }, $$\n",
        "in other words:\n",
        "$$ \\frac{ \\text{TN} }{ \\text{TN + FP} } = \\frac{9995}{9995 + 0}. $$\n",
        "  <br>\n",
        "\n",
        "We also noted that if we only look at the cases that are actually positive, then the model correctly predictes 0% of them!  Here, \"0%\" is computed as follows\n",
        "$$ \\frac{ \\text{number of all positive cases that are correctly predicted as positive} }{ \\text{number of all cases that are actually positive} }, $$\n",
        "in other words:\n",
        "$$ \\frac{ \\text{TP} }{ \\text{TP + FN} } = \\frac{0}{0+5}. $$\n",
        "\n",
        "*****\n",
        "\n",
        "\n",
        "The above two metrics are often referred to as \"specificity\" and \"recall\", respectively.  There are also other metrics that we can compute in terms of TP, FP, TN, and FN.  Below are some of them; you can find others [here](https://en.wikipedia.org/wiki/Confusion_matrix#Table_of_confusion).\n",
        "\n",
        "**Specificity** <br>(also known as \"selectivity\" or \"True Negative Rate\")\n",
        "$$ \\frac{ \\text{TN} }{ \\text{TN + FP} }, $$\n",
        "the ratio of correct predictions among all observations that are actually negative.\n",
        "\n",
        "**Recall** <br>(also known as \"sensitivity\", \"hit rate\", or \"True Positive Rate\")\n",
        "$$ \\frac{ \\text{TP} }{ \\text{TP + FN} } ,$$\n",
        "the ratio of correct predictions among all observations that are actually positive.\n",
        "\n",
        "**Precision** <br>\n",
        "$$ \\frac{ \\text{TP} }{ \\text{TP + FP} }, $$\n",
        "the ratio of correct predictions among all observations that are predicted to be positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxrhB9z-rEoV"
      },
      "source": [
        "***Example***\n",
        "\n",
        "Let us now return to our main example of classifying tumors as benign or malignant.  We have previously computed the accuracy of our model on the training dataset.  Let us now compute the specificity, recall, and precision of our model on the training dataset.  To do this, we first need to count the number of true positives, true negatives, false positives, and false negatives in our predictions.\n",
        "\n",
        "\n",
        "To do this, we will first setup a data frame (let's call it `training_results_df`) that has 6 columns: 'y_train', 'y_train_predicted', 'TP', 'TN', 'FP', and 'FN':"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "xm4CY1GErEoV"
      },
      "outputs": [],
      "source": [
        "training_results_df = pd.DataFrame( {'y_train': y_train, # this column contains the actual y values in the training data\n",
        "                                     'y_train_predicted': y_train_predicted, # this column contains the predicted y values\n",
        "                                     'TP': np.zeros( len(y_train) ), # fill this column with zeros for now \n",
        "                                     'TN': np.zeros( len(y_train) ), # fill this column with zeros for now\n",
        "                                     'FP': np.zeros( len(y_train) ), # fill this column with zeros for now\n",
        "                                     'FN': np.zeros( len(y_train) ) # fill this column with zeros for now \n",
        "                                    })\n",
        "\n",
        "# preview the first few rows:\n",
        "training_results_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z98mq_t4rEoV"
      },
      "source": [
        "Then, we will go through each row of this data frame to check whether the row corresponds to a true positive, true negative, false positive, or false negative.  We will then fill in a 1 to replace the zero in the appropriate column.\n",
        "\n",
        "\n",
        "Recall that 2 indicates that the tumor is benign and 4 indicates that the tumor is malignant.  There is some freedom on how to interpret which of the two outcome is \"positive\" and which is \"negative\", depending on context.  In this example, let us follow the convention of that a positive diagnosis is the presence of an illness; that is, **we will interpret 4/malignant as a positive case** (\"the diagnosis of malignant cancer is positive\", an undesireable outcome) and **2/benign as a negative case**.\n",
        "\n",
        "For example, the case the first row (index 146) is actually positive but is predicted to be negative.  Therefore, this is a false negative result.  We will seek to enter the number 1 in the FN column of this row, leaving the other three columns as zeros.\n",
        "\n",
        "We will use a for loop to go through each row of this data frame.  For each row, we will check which of the four cases is satisfied, using if/else statements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8x7HLuCOrEoV"
      },
      "outputs": [],
      "source": [
        "# we will go through each row of the dataframe by looping over its row indices\n",
        "for i in training_results_df.index : \n",
        "    actual_value = training_results_df.loc[i, 'y_train']      # get value in row index i, column y_train\n",
        "    predicted_value = training_results_df.loc[i, 'y_train_predicted']  # get value in row index i, column y_train_predicted\n",
        "    \n",
        "    if actual_value == 4 and predicted_value == 4:   # actual is positive, predicted is positive --> True Positive\n",
        "        training_results_df.loc[i, 'TP'] = 1    # Set TP to 1; leave other values at 0\n",
        "    elif actual_value == 4 and predicted_value == 2: # actual is positive, predicted is negative --> False Negative\n",
        "        training_results_df.loc[i, 'FN'] = 1    # Set FN to 1; leave other values at 0\n",
        "    elif actual_value == 2 and predicted_value == 4: # actual is negative, predicted is positive --> False Positive\n",
        "        training_results_df.loc[i, 'FP'] = 1    # Set FP to 1; leave other values at 0\n",
        "    elif actual_value == 2 and predicted_value == 2: # actual is negative, predicted is negative --> True Negative\n",
        "        training_results_df.loc[i, 'TN'] = 1    # Set TN to 1; leave other values at 0\n",
        "        \n",
        "# preview the updated dataframe\n",
        "training_results_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khQ9crNrrEoW"
      },
      "source": [
        "Then, to find the total number of true positives, true negatives false positives, and false negatives, we take the sum of each of these columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWXRhBmXrEoW"
      },
      "outputs": [],
      "source": [
        "num_tp = np.sum(training_results_df['TP'])\n",
        "num_tn = np.sum(training_results_df['TN'])\n",
        "num_fp = np.sum(training_results_df['FP'])\n",
        "num_fn = np.sum(training_results_df['FN'])\n",
        "\n",
        "# display the numbers\n",
        "print( num_tp )\n",
        "print( num_tn )\n",
        "print( num_fp )\n",
        "print( num_fn )\n",
        "\n",
        "# check: the sum of the above four numbers should be equal to \n",
        "#  the number of observations ( which is also the number of rows of this dataframe)\n",
        "print( num_tp + num_tn + num_fp + num_fn )\n",
        "print( training_results_df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4zC8WNDrEoW"
      },
      "source": [
        "Finally, let us compute the accuracy, precision, and recall of the model on this training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-yQWYsKrEoW"
      },
      "outputs": [],
      "source": [
        "accuracy = ( num_tp + num_tn ) / ( num_tp + num_tn + num_fp + num_fn )\n",
        "precision = ( num_tp ) / ( num_tp + num_fp )\n",
        "recall = ( num_tp  ) / ( num_tp + num_fn )\n",
        "\n",
        "print('Accuracy:')\n",
        "print(accuracy)\n",
        "print('Precision:')\n",
        "print(precision)\n",
        "print('Recall:')\n",
        "print(recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guTeQPS8rEoX"
      },
      "source": [
        "We see that our model has a relatively high precision of about 96.2\\%, which means that among all observations in the training dataset that are malignant tumors, the model correctly identify them 96.2\\% of the time.\n",
        "\n",
        "On the other hand, the model has a relatively low recall of about 60.7\\%, which means that among all observations in the training dataset that are predicted as malignant tumor, the model only gets 60.7\\% of them right.  That is, about 49.3\\% of those prediced as malignant are actually benign.\n",
        "\n",
        "These two additional pieces of information was not gleaned when we only computed the overall accuracy of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoS6LVDgrEoX"
      },
      "source": [
        "## Lesson 5 Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joWyOmY1rEoX"
      },
      "source": [
        "***Exercise 1***\n",
        "\n",
        "The code cell below summarizes the codes used to compute the accuracy, precision, and recall of the model on the **training** dataset.\n",
        "\n",
        "Please modify this code cell to compute the accuracy, precision, and recall of the model on the **test** dataset.  How do the accuracy, precision, and recall on the test dataset compare to these metrics on the training dataset?  What does this say about the model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY4f-OPVrEoX"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Use the predict_cancer_class() function to make a prediction on the test dataset\n",
        "# TODO: modify the codes below to do this on the test dataset (currently it's for the training dataset)\n",
        "\n",
        "y_train_predicted = np.zeros( y_train.shape ).astype(int)\n",
        "y_train_predicted = pd.Series(y_train_predicted)\n",
        "y_train_predicted.index = y_train.index\n",
        "\n",
        "# make a prediction for the value of class, for each row in the training dataset.\n",
        "for index in X_train.index :\n",
        "    y_train_predicted[index] = predict_cancer_class( X_train['clump_thickness'][index] , X_train['uniformity_cellsize'][index] )\n",
        "\n",
        "    \n",
        "\n",
        "# STEP 2: Take each predictions from STEP 1 and count the number of true positives, true negatives, etc.\n",
        "# TODO: modify the codes below to do this on the test dataset (currently it's for the training dataset)\n",
        "\n",
        "# first set up a placeholder dataframe\n",
        "training_results_df = pd.DataFrame( {'y_train': y_train, # this column contains the actual y values in the training data\n",
        "                                     'y_train_predicted': y_train_predicted, # this column contains the predicted y values\n",
        "                                     'TP': np.zeros( len(y_train) ), # fill this column with zeros for now \n",
        "                                     'TN': np.zeros( len(y_train) ), # fill this column with zeros for now\n",
        "                                     'FP': np.zeros( len(y_train) ), # fill this column with zeros for now\n",
        "                                     'FN': np.zeros( len(y_train) ) # fill this column with zeros for now \n",
        "                                    })\n",
        "\n",
        "\n",
        "# we will go through each row of the dataframe by looping over its row indices\n",
        "for i in training_results_df.index : \n",
        "    actual_value = training_results_df.loc[i, 'y_train']      # get value in row index i, column y_train\n",
        "    predicted_value = training_results_df.loc[i, 'y_train_predicted']  # get value in row index i, column y_train_predicted\n",
        "    \n",
        "    if actual_value == 4 and predicted_value == 4:   # actual is positive, predicted is positive --> True Positive\n",
        "        training_results_df.loc[i, 'TP'] = 1    # Set TP to 1; leave other values at 0\n",
        "    elif actual_value == 4 and predicted_value == 2: # actual is positive, predicted is negative --> False Negative\n",
        "        training_results_df.loc[i, 'FN'] = 1    # Set FN to 1; leave other values at 0\n",
        "    elif actual_value == 2 and predicted_value == 4: # actual is negative, predicted is positive --> False Positive\n",
        "        training_results_df.loc[i, 'FP'] = 1    # Set FP to 1; leave other values at 0\n",
        "    elif actual_value == 2 and predicted_value == 2: # actual is negative, predicted is negative --> True Negative\n",
        "        training_results_df.loc[i, 'TN'] = 1    # Set TN to 1; leave other values at 0\n",
        "        \n",
        "\n",
        "# for each column of the above dataframe, take the sum\n",
        "num_tp = np.sum(training_results_df['TP'])\n",
        "num_tn = np.sum(training_results_df['TN'])\n",
        "num_fp = np.sum(training_results_df['FP'])\n",
        "num_fn = np.sum(training_results_df['FN'])\n",
        "\n",
        "\n",
        "# check: the sum of the above four numbers should be equal to \n",
        "#  the number of observations ( which is also the number of rows of this dataframe)\n",
        "print( num_tp + num_tn + num_fp + num_fn )\n",
        "print( training_results_df.shape[0])\n",
        "\n",
        "# STEP 3: Use the above numbers to compute the accuracy, precision, and recall \n",
        "# TODO: modify the codes below to do this on the test dataset (currently it's for the training dataset)\n",
        "\n",
        "accuracy = ( num_tp + num_tn ) / ( num_tp + num_tn + num_fp + num_fn )\n",
        "precision = ( num_tp ) / ( num_tp + num_fp )\n",
        "recall = ( num_tp  ) / ( num_tp + num_fn )\n",
        "\n",
        "print('Accuracy:')\n",
        "print(accuracy)\n",
        "print('Precision:')\n",
        "print(precision)\n",
        "print('Recall:')\n",
        "print(recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzRJWUAYrEoY"
      },
      "source": [
        "***Exercise 2***\n",
        "\n",
        "We created a prediction function called `predict_cancer_class` above, whose codes we reproduced below\n",
        "```\n",
        "def predict_cancer_class( clump_thickness, predicted_class ):\n",
        "    if clump_thickness <= 6 :\n",
        "        if uniformity_cellsize <= 4:\n",
        "            predicted_class = 2\n",
        "        else:\n",
        "            predicted_class = 4\n",
        "    else:\n",
        "        predicted_class = 4\n",
        "        \n",
        "    return( predicted_class )\n",
        "```\n",
        "Recall that we came up with this function based on patterns we observed as we explore the training dataset.\n",
        "\n",
        "\n",
        "Please explore the training dataset to find other patterns that can be used to create a different prediction model or to modify the above function. Then, write the new function definition in the code cell below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vweX-LTerEoY"
      },
      "outputs": [],
      "source": [
        "# explore the training data here\n",
        "# you can add additional/new code cells as necessary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "venm-aHLrEoY"
      },
      "outputs": [],
      "source": [
        "# explore the training data here\n",
        "# you can add additional/new code cells as necessary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29agOe2MrEoZ"
      },
      "outputs": [],
      "source": [
        "# TODO: Write a new prediction function below \n",
        "\n",
        "def predict_cancer_class_newmodel(  .... ) :\n",
        "    # ...\n",
        "    # ...\n",
        "\n",
        "\n",
        "\n",
        "## Assess the performance of your new model\n",
        "\n",
        "# STEP 1: Use the predict_cancer_class() function to make a prediction on the test dataset\n",
        "\n",
        "y_train_predicted = np.zeros( y_train.shape ).astype(int)\n",
        "y_train_predicted = pd.Series(y_train_predicted)\n",
        "y_train_predicted.index = y_train.index\n",
        "\n",
        "# make a prediction for the value of class, for each row in the training dataset.\n",
        "for index in X_train.index :   \n",
        "    # MODIFY THE LINE BELOW TO USE YOUR NEW PREDICTION FUNCTION\n",
        "    y_train_predicted[index] = predict_cancer_class_newmodel( X_train['COLNAME1'][index] , X_train['COLNAME2'][index] )\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "## You do not have to modify the code cell below this line\n",
        "## --------------------------------\n",
        "# STEP 2: Take each predictions from STEP 1 and count the number of true positives, true negatives, etc.\n",
        "\n",
        "# first set up a placeholder dataframe\n",
        "training_results_df = pd.DataFrame( {'y_train': y_train, # this column contains the actual y values in the training data\n",
        "                                     'y_train_predicted': y_train_predicted, # this column contains the predicted y values\n",
        "                                     'TP': np.zeros( len(y_train) ), # fill this column with zeros for now \n",
        "                                     'TN': np.zeros( len(y_train) ), # fill this column with zeros for now\n",
        "                                     'FP': np.zeros( len(y_train) ), # fill this column with zeros for now\n",
        "                                     'FN': np.zeros( len(y_train) ) # fill this column with zeros for now \n",
        "                                    })\n",
        "\n",
        "\n",
        "# we will go through each row of the dataframe by looping over its row indices\n",
        "for i in training_results_df.index : \n",
        "    actual_value = training_results_df.loc[i, 'y_train']      # get value in row index i, column y_train\n",
        "    predicted_value = training_results_df.loc[i, 'y_train_predicted']  # get value in row index i, column y_train_predicted\n",
        "    \n",
        "    if actual_value == 4 and predicted_value == 4:   # actual is positive, predicted is positive --> True Positive\n",
        "        training_results_df.loc[i, 'TP'] = 1    # Set TP to 1; leave other values at 0\n",
        "    elif actual_value == 4 and predicted_value == 2: # actual is positive, predicted is negative --> False Negative\n",
        "        training_results_df.loc[i, 'FN'] = 1    # Set FN to 1; leave other values at 0\n",
        "    elif actual_value == 2 and predicted_value == 4: # actual is negative, predicted is positive --> False Positive\n",
        "        training_results_df.loc[i, 'FP'] = 1    # Set FP to 1; leave other values at 0\n",
        "    elif actual_value == 2 and predicted_value == 2: # actual is negative, predicted is negative --> True Negative\n",
        "        training_results_df.loc[i, 'TN'] = 1    # Set TN to 1; leave other values at 0\n",
        "        \n",
        "\n",
        "# for each column of the above dataframe, take the sum\n",
        "num_tp = np.sum(training_results_df['TP'])\n",
        "num_tn = np.sum(training_results_df['TN'])\n",
        "num_fp = np.sum(training_results_df['FP'])\n",
        "num_fn = np.sum(training_results_df['FN'])\n",
        "\n",
        "\n",
        "# check: the sum of the above four numbers should be equal to \n",
        "#  the number of observations ( which is also the number of rows of this dataframe)\n",
        "print( num_tp + num_tn + num_fp + num_fn )\n",
        "print( training_results_df.shape[0])\n",
        "\n",
        "# STEP 3: Use the above numbers to compute the accuracy, precision, and recall \n",
        "\n",
        "accuracy = ( num_tp + num_tn ) / ( num_tp + num_tn + num_fp + num_fn )\n",
        "precision = ( num_tp ) / ( num_tp + num_fp )\n",
        "recall = ( num_tp  ) / ( num_tp + num_fn )\n",
        "\n",
        "print('Accuracy:')\n",
        "print(accuracy)\n",
        "print('Precision:')\n",
        "print(precision)\n",
        "print('Recall:')\n",
        "print(recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYZwUuiarEoZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssyBF0D0rEoa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}